{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1: Intro to Convolutional Neural Networks for CV\n",
    "\n",
    "* Block World - Larry Roberts writes first CV paper organizing into geometric shapes\n",
    "* Vision - David Marr book describes process where we can translate an input image into a 3-D model\n",
    "* 1987 Paper - David Lowe tried to construct image from lines and edges\n",
    "* If object recognition is too difficult, do image segmentation to seperate pixels.\n",
    "* Face Detection 2001 - Viola & Jones developed real-time facial recogntion\n",
    "* SIFT Paper from 1999 - David Lowe utilized several key diagnostic features to match objects\n",
    "* Deformable Part Model - Human body recognition\n",
    "* 1998 Paper - LeCun makes CNN for digit recognition\n",
    "* 2012 Paper - Krizhevsky wins ImageNet with CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2: Image Classification\n",
    "\n",
    "* Image Classification key to CV but no clear algorithm \n",
    "    - Semantic Gap - Labels we assign are meaningless to computer\n",
    "    - Illumination - Lighting conditions\n",
    "    - Deformation - Object can be distorted \n",
    "    - Occlusion - Object can be partially obscured\n",
    "    - Background Clutter - There may be many objects\n",
    "    - Intraclass Variation - A class may describe a variety of appearances\n",
    "* Data-driven approach with Train -> Predict\n",
    "\n",
    "        def train(self, X, y):\n",
    "            # X is NxD where each row is example. Y is 1-d of size N\n",
    "            self.Xtr = X\n",
    "            self.ytr = y\n",
    "        def predict(self, X):\n",
    "            # X is NxD where each row is example we wish to precict label\n",
    "            num_test = X.shate[0]\n",
    "            Ypred = np.zeros(num_test, dtype = self.ytr.dtype)\n",
    "            for i in xrange(num_test):\n",
    "                distances = np.sum(mp.ans(self.Xtr - X[i,:]), axis = 1) # Use L1 distance\n",
    "                min_index = np.argmin(distances) #Get index with smallest dist\n",
    "                Ypred[i] = self.ytr[min_index] #Predict label of nearest example\n",
    "        return Ypred\n",
    "        \n",
    "    - NN example bad because slow predict with fast train\n",
    "    - KNN allows to reduce noise with majority vote\n",
    "    - L1 distance square that depends on coordinate system\n",
    "    - L2 (Euclidian) distance is a circle\n",
    "    - KNN almost never used on images as it must be dense, making it exponential\n",
    "* Hyperparamater (choices we set for alg) - K or distance values\n",
    "    - Don't choose hyperparameter that works best on data as it can vastly overfit\n",
    "    - Don't hyperparameters from both train and test sets as it will not be testable\n",
    "    - Split data into train, val, and test sets\n",
    "    - For small datasets can split data into multiple folds\n",
    "* Linear Classification \n",
    "    - Summarize data into parameters or weights \n",
    "    - f(x,W) simplest form is Wx + b\n",
    "    - Only finds one template so averages out\n",
    "    - Struggles with seperated or multimodal images\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3: Loss Functions and Optimization\n",
    "\n",
    "* Loss Functions - How to determine how wrong the classification is\n",
    "    - Given a dataset, loss is the sum of loss over examples\n",
    "    *Insert LATEX FUNCTION*\n",
    "    - Multiclass SVM loss: Hinge loss decreases linearly as score of true class increases until margin - We expect a loss of C-1 to start\n",
    "    \n",
    "            def L_i_vectorized(x, y, W):\n",
    "                scores = W.dot(x)\n",
    "                margins = np.maximum(0, scores - scores[y] + 1)\n",
    "                margins[y] = 0\n",
    "                loss_i = np.sum(margins)\n",
    "                return loss_i\n",
    "                \n",
    "    - Softmax Classifier exponentiates (make positive) and normalize (sum to one) - We expect a loss of log(C) to start\n",
    "                \n",
    "* We want to change it based on the test data and avoid overfitting uzing regularization (simplifies model)\n",
    "    - L2 regularization common penalizes euclidean norm *LATEX* \n",
    "    - L1, Elastic net, Max norm, Dropout are also used\n",
    "\n",
    "* Optimization seeking to reach a min value\n",
    "    - Bad solution: Random search\n",
    "    - Iterate down the slope - the derivative of the function\n",
    "    - The gradient is the vector while the slope in any direction is the dot product of the direction with gradient\n",
    "    - The loss is a function of W, take analytic gradient\n",
    "    \n",
    "            while True: \n",
    "                weights_grad = evaluate_gradient(loss_fun, data, weights)\n",
    "                weights += - step_size * weights_grad\n",
    "            \n",
    "    - In Gradient Descent, step size is key hyperparameter\n",
    "    - Full sum is expensive so use Stochastic Gradient Descent, use minibatch\n",
    "        \n",
    "            while True:\n",
    "                data_batch = sample_training_data(data, 256) #samples 256 examples\n",
    "                weights_grad = evaluate_gradient(loss_fun, data, weights)\n",
    "                weights += - step_size * weights_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
